{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"V100","machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.11"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"olVpflosFGpf","pycharm":{"name":"#%% md\n"}},"source":[]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","\n","!nvidia-smi"],"metadata":{"id":"vrrx4wDb-MuE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import necessary libraries\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import T5ForConditionalGeneration, T5Tokenizer\n","from sklearn.metrics import classification_report, accuracy_score\n","from tqdm import tqdm\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","\n","# Assuming df is your DataFrame containing the 'tmu_gfm_dataset'\n","data = pd.read_csv('/content/drive/MyDrive/project/Jfleg4-2-4.csv').dropna()\n","train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","#  Model Training\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(device)\n","tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")"],"metadata":{"id":"KD1VeGCWZpuM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Data Acquisition\n","class GrammarCorrectionDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=128):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        input_text = self.data.iloc[idx]['output']\n","        error_text = self.data.iloc[idx]['source']\n","\n","        input_encoding = tokenizer.encode_plus(\n","            input_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        target_encoding = tokenizer.encode(\n","            error_text,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': input_encoding['input_ids'].squeeze(0),\n","            'attention_mask': input_encoding['attention_mask'].squeeze(0),\n","            'labels': target_encoding.squeeze(0)\n","        }\n"],"metadata":{"id":"n9tux9v9Zvfs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create training and validation sets\n","train_dataset = GrammarCorrectionDataset(train_data, tokenizer)\n","val_dataset = GrammarCorrectionDataset(val_data, tokenizer)\n","\n","# DataLoader设置\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n"],"metadata":{"id":"lrqPAlJJaY9D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 模型训练\n","epochs = 10\n","batch_size = 8\n","learning_rate = 2e-5\n","\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","for epoch in range(epochs):\n","    model.train()\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","    model.eval()\n","    total_loss = 0\n","    total_samples = 0\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item() * input_ids.size(0)\n","            total_samples += input_ids.size(0)\n","\n","    avg_loss = total_loss / total_samples\n","    print(f'Epoch {epoch+1}/{epochs}, Validation Loss: {avg_loss:.4f}')\n","\n","# # Plot confusion matrix\n","# cm = confusion_matrix(list(plot_dict.values()), list(plot_dict.keys()))\n","# sns.heatmap(cm, annot=True)\n","\n","# # Step 5: Show the result\n","# print(classification_report(list(plot_dict.values()), list(plot_dict.keys())))\n","# print(\"Accuracy:\", accuracy_score(list(plot_dict.values()), list(plot_dict.keys())))"],"metadata":{"id":"i-iaPwkpYdJP"},"execution_count":null,"outputs":[]}]}