{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNb8nDGuH0AIBVIXqZiLka6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sl-Zhou/NLP-project/blob/armin/KD_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!nvidia-smi\n",
        "!pip install datasets==2.11.0 --quiet\n",
        "!pip install accelerate  --quiet\n",
        "!pip install evaluate   --quiet\n",
        "!pip install rouge_score --quiet\n",
        "# Data Preparation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datasets\n",
        "data = pd.read_csv('./data/Jfleg4-2-4.csv')\n",
        "data.head()\n",
        "# Define the output JSONL file name\n",
        "filename = './data/output.jsonl'\n",
        "\n",
        "# Iterate through the rows and write each row as a JSON object to the JSONL file\n",
        "with open(filename, 'w') as jsonl_file:\n",
        "    for _, row in data.iterrows():\n",
        "        json_data = row.to_json(orient='columns')\n",
        "        jsonl_file.write(json_data + '\\n')\n",
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\")\n",
        "\n",
        "# Define tokenize function\n",
        "def tokenize_function(examples):\n",
        "    # Concatenate text pairs if necessary\n",
        "    if \"input\" in examples and \"target\" in examples:  # Assuming the presence of 'input' and 'target' in your dataset\n",
        "        text = [inp + tgt for inp, tgt in zip(examples[\"input\"], examples[\"target\"])]\n",
        "    else:\n",
        "        text = examples[\"text\"]\n",
        "\n",
        "    # Tokenize batch of text\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        padding='max_length',  # Will pad to the max length in the batch or max_length if provided\n",
        "        truncation=True,\n",
        "        max_length=128,  # You can define max length here\n",
        "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Load dataset\n",
        "finetuning_dataset_loaded = datasets.load_dataset(\"json\", data_files=filename, split=\"train\")\n",
        "\n",
        "# Apply tokenize function\n",
        "tokenized_dataset = finetuning_dataset_loaded.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1,  # You can set a larger batch size for efficiency\n",
        ")\n",
        "\n",
        "# Add labels\n",
        "tokenized_dataset = tokenized_dataset.add_column(\"labels\", tokenized_dataset[\"input_ids\"])\n",
        "\n",
        "\n",
        "print(tokenized_dataset[\"input\"][0])\n",
        "\n",
        "print(tokenized_dataset[\"target\"][0])\n",
        "\n",
        "print(tokenized_dataset[\"input_ids\"][0])\n",
        "\n",
        "# Train test split\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, shuffle=True, seed=123)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "test_dataset = split_dataset[\"test\"]\n",
        "\n",
        "print(split_dataset)\n",
        "# Before Training\n",
        "import datasets\n",
        "import logging\n",
        "import random\n",
        "import logging\n",
        "import torch\n",
        "import transformers\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForCausalLM\n",
        "from transformers import TrainingArguments, Trainer\n",
        "model_name = \"EleutherAI/pythia-70m\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "base_model.to(device)\n",
        "print(device)\n",
        "test_text = test_dataset[1]['input']\n",
        "max_input_tokens = 1000\n",
        "max_output_tokens=100\n",
        "# Tokenize\n",
        "input_ids = tokenizer.encode(\n",
        "      test_text,\n",
        "      return_tensors=\"pt\",\n",
        "      truncation=True,\n",
        "      max_length=max_input_tokens\n",
        ")\n",
        "\n",
        "# Generate\n",
        "device = base_model.device\n",
        "generated_tokens_with_prompt = base_model.generate(input_ids=input_ids.to(device), max_length=max_output_tokens)\n",
        "\n",
        "# Decode\n",
        "generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "# Strip the prompt\n",
        "generated_text_answer = generated_text_with_prompt[0][len(test_text):]\n",
        "\n",
        "\n",
        "print(\"input:\", test_text)\n",
        "print(f\"Correct answer from dataset: {test_dataset[1]['target']}\")\n",
        "print(\"Model's answer: \")\n",
        "print(generated_text_answer)\n",
        "\n",
        "# finetuning\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import DataCollatorWithPadding\n",
        "# number of epoch\n",
        "max_steps = -1\n",
        "\n",
        "# Save model to this direction\n",
        "trained_model_name = f\"pythia_ft_{max_steps}_steps\"\n",
        "output_dir = '/content/drive/MyDrive/project/' + trained_model_name\n",
        "save_dir = f'{output_dir}/final'\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "\n",
        "  # Learning rate\n",
        "  learning_rate=1.0e-5,\n",
        "\n",
        "  # Number of training epochs\n",
        "  num_train_epochs=3,\n",
        "\n",
        "  # Max steps to train for (each step is a batch of data)\n",
        "  # Overrides num_train_epochs, if not -1\n",
        "  max_steps=max_steps,\n",
        "\n",
        "  # Batch size for training\n",
        "  per_device_train_batch_size=64,\n",
        "\n",
        "  # Directory to save model checkpoints\n",
        "  output_dir=output_dir,\n",
        "\n",
        "  # Other arguments\n",
        "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
        "  disable_tqdm=False, # Disable progress bars\n",
        "  eval_steps=120, # Number of update steps between two evaluations\n",
        "  save_steps=120, # After # steps model is saved\n",
        "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
        "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
        "  evaluation_strategy=\"steps\",\n",
        "  save_strategy=\"steps\",\n",
        "  logging_strategy=\"steps\",\n",
        "  logging_steps=1,\n",
        "  optim=\"adafactor\",\n",
        "  gradient_accumulation_steps = 4,\n",
        "  gradient_checkpointing=False,\n",
        "\n",
        "  # Parameters for early stopping\n",
        "  load_best_model_at_end=True,\n",
        "  save_total_limit=1,\n",
        "  metric_for_best_model=\"eval_loss\",\n",
        "  greater_is_better=False\n",
        ")\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=None)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "\n",
        "training_output = trainer.train()\n",
        "\n",
        "trainer.save_model(save_dir)\n",
        "print(\"Saved model to:\", save_dir)\n",
        "device_count = torch.cuda.device_count()\n",
        "if device_count > 0:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "save_dir = '/content/drive/MyDrive/project/pythia_ft_-1_steps/final'\n",
        "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
        "finetuned_slightly_model.to(device)\n",
        "# def generate_output(test_question, model, max_input_length, max_new_tokens):\n",
        "#     # Tokenize\n",
        "#     input_ids = tokenizer.encode(\n",
        "#         test_question,\n",
        "#         return_tensors=\"pt\",\n",
        "#         truncation=True,\n",
        "#         max_length=max_input_length  # Make sure to set a proper max_length\n",
        "#     )\n",
        "\n",
        "#     # Generate\n",
        "#     device = model.device\n",
        "#     generated_tokens_with_prompt = model.generate(\n",
        "#         input_ids=input_ids.to(device),\n",
        "#         max_new_tokens=max_new_tokens  # Set max_new_tokens to control the number of generated tokens\n",
        "#     )\n",
        "\n",
        "#     # Decode\n",
        "#     generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "\n",
        "#     # Strip the prompt (if necessary)\n",
        "#     if test_question in generated_text_with_prompt[0]:\n",
        "#         generated_text_answer = generated_text_with_prompt[0].replace(test_question, '')\n",
        "#     else:\n",
        "#         generated_text_answer = generated_text_with_prompt[0]\n",
        "#     return generated_text_answer.strip()\n",
        "\n",
        "model_name = \"EleutherAI/pythia-70m\"\n",
        "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "base_model.to(device)\n",
        "finetuned_slightly_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True)\n",
        "finetuned_slightly_model.to(device)\n",
        "from tqdm import tqdm\n",
        "\n",
        "max_input_tokens = 1000\n",
        "max_output_tokens = 400\n",
        "text_list = []\n",
        "tuned_predicted_text_list = []\n",
        "actual_test_list = []\n",
        "base_predicted_text_list = []\n",
        "\n",
        "\n",
        "for i in tqdm(range(100), desc=\"Processing test dataset\"):\n",
        "    test_text = test_dataset[i]['input']\n",
        "    # Tokenize\n",
        "    input_ids = tokenizer.encode(\n",
        "        test_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_input_tokens\n",
        "    )\n",
        "\n",
        "    # Generate\n",
        "    device = base_model.device\n",
        "    generated_tokens_with_prompt = base_model.generate(input_ids=input_ids.to(device), max_length=max_output_tokens)\n",
        "    generated_tokens_with_prompt_ft = finetuned_slightly_model.generate(input_ids=input_ids.to(device), max_length=max_output_tokens)\n",
        "\n",
        "    # Decode\n",
        "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
        "    generated_text_with_prompt_ft = tokenizer.batch_decode(generated_tokens_with_prompt_ft, skip_special_tokens=True)\n",
        "\n",
        "    # Strip the prompt\n",
        "    generated_text_answer = generated_text_with_prompt[0][len(test_text):]\n",
        "    generated_text_answer_ft = generated_text_with_prompt_ft[0][len(test_text):]\n",
        "\n",
        "    # Append results to lists\n",
        "    actual_test_list.append(test_dataset[i]['target'])\n",
        "    tuned_predicted_text_list.append(generated_text_answer_ft)\n",
        "    base_predicted_text_list.append(generated_text_answer)\n",
        "    text_list.append(test_text)\n",
        "\n",
        "\n",
        "print(\"Data processing complete.\")\n",
        "\n",
        "# Create a DataFrame from the lists\n",
        "data = {\n",
        "    'input': text_list,\n",
        "    'correct': actual_test_list,\n",
        "    'Tuned Prediction': tuned_predicted_text_list,\n",
        "    'Base Prediction': base_predicted_text_list\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Write the DataFrame to a CSV file\n",
        "df.to_csv('/content/drive/MyDrive/project/dataset_ft_100.csv', index=False)\n",
        "\n",
        "import pandas as pd\n",
        "import evaluate\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/project/dataset_ft_100.csv')\n",
        "\n",
        "\n",
        "base_predicted_text_list = df['Base Prediction'].tolist()\n",
        "tuned_predicted_text_list = df['Tuned Prediction'].tolist()\n",
        "actual_test_list = df['correct'].tolist()\n",
        "\n",
        "\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "print(\"Base Model Predictions BLEU Results:\")\n",
        "base_bleu_results = bleu.compute(predictions=base_predicted_text_list, references=[[ref] for ref in actual_test_list])\n",
        "print(base_bleu_results)\n",
        "\n",
        "print(\"Fine Tuned Model Predictions BLEU Results:\")\n",
        "tuned_bleu_results = bleu.compute(predictions=tuned_predicted_text_list, references=[[ref] for ref in actual_test_list])\n",
        "print(tuned_bleu_results)\n",
        "\n",
        "print(\"Base Model Predictions ROUGE Results:\")\n",
        "base_rouge_results = rouge.compute(predictions=base_predicted_text_list, references=actual_test_list)\n",
        "print(base_rouge_results)\n",
        "\n",
        "print(\"Fine Tuned Model Predictions ROUGE Results:\")\n",
        "tuned_rouge_results = rouge.compute(predictions=tuned_predicted_text_list, references=actual_test_list)\n",
        "print(tuned_rouge_results)\n"
      ],
      "metadata": {
        "id": "1rwDwiiGTfcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}